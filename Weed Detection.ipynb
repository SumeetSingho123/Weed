{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Importing Some Basic Libraries**"},{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport os\npath1 = r'C:\\Users\\hp\\Desktop\\Capstone project\\dataset'\nprint(os.listdir(path1))\n\nimport sys\npathcv = r'C:\\Users\\hp\\Anaconda3\\envs\\opencv-env\\Lib\\site-packages'\nsys.path.append(pathcv)\n\nimport cv2\nfrom sklearn import utils\n\nfrom skimage.transform import rescale, resize\nimport tensorflow as tf\n\nfrom keras import backend as K\nfrom keras.preprocessing.image import load_img\nfrom keras.models import Model\nfrom keras.layers import *\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Defining Image Path and Assigning Labels.**"},{"metadata":{"trusted":false},"cell_type":"code","source":"def image_path(img_type, img_number):\n    \n    #image_type: 'images' #original image, 'annotations' #crop-weed label, 'mask' #vegetation segmentation\n    #image_number: the number on the image name\n    \n    image_name = img_type[:-1]\n    if img_number < 10:\n        path = 'C:/Users/hp/Desktop/Capstone project/dataset/'+str(img_type)+'/00'+str(img_number)+'_'+str(image_name)+'.png'\n    else:\n        path = 'C:/Users/hp/Desktop/Capstone project/dataset/'+str(img_type)+'/0'+str(img_number)+'_'+str(image_name)+'.png'\n    return path\n\n# generating labels for each image\ndef label_generator(number):\n    annotation = cv2.imread(image_path('annotations', number))\n    height = annotation.shape[0]\n    width = annotation.shape[1]\n   # channel = annotation.shape[2]\n    labels = np.zeros((height, width, 3))\n    for i in range(height):\n        for j in range(width):\n            if np.all(annotation[i,j,:] == np.array([0,255,0])):\n                labels[i,j,0] = 1\n            elif np.all(annotation[i,j,:] == np.array([0,0,255])):\n                labels[i,j,1] = 1\n            elif np.all(annotation[i,j,:] == np.array([0,0,0])):\n                labels[i,j,2] = 1\n    return labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# rescaling the image\n\n# we rescale the image so as to get a smaller image, which reduces our calculations and in longer run also reduces \n# time complexity and computations.\n\nimage = cv2.imread(image_path('annotations',1))\nimage_rescaled = rescale(image, 1.0 / 6.0, anti_aliasing=True)\nplt.imshow(image_rescaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Load two images\nimg1 = cv2.imread(image_path('images',1))\nimg2 = cv2.imread(image_path('masks',1))\n\n# I want to put logo on top-left corner, So I create a ROI\nrows,cols,channels = img2.shape\nroi = img1[0:rows, 0:cols ]\n\n# Now create a mask of logo and create its inverse mask also\nimg2gray = cv2.cvtColor(img2,cv2.COLOR_BGR2GRAY)\nret, mask = cv2.threshold(img2gray, 10, 255, cv2.THRESH_BINARY)\nmask_inv = cv2.bitwise_not(mask)\n\n# Now black-out the area of logo in ROI\nimg1_bg = cv2.bitwise_and(roi,roi,mask = mask_inv)\n\n# Take only region of logo from logo image.\nimg2_fg = cv2.bitwise_and(img2,img2,mask = mask)\n\n# Put logo in ROI and modify the main image\ndst = cv2.add(img1_bg,img2_fg)\nimg1[0:rows, 0:cols ] = dst\nplt.imshow(img1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"label = label_generator(1)\nplt.figure()\n\n#plt.imshow(label)\nfor i in range(3):\n    plt.subplot(1,3,i+1)\n    plt.imshow(label[:,:,i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"x_train = np.zeros((120, 161, 216, 3))\ny_train = np.zeros((120, 161, 216, 3))\nx_test = np.zeros((20, 161, 216, 3))\ny_test = np.zeros((20, 161, 216, 3))\n\nplt.figure(figsize=(8,5))\n\nfor i in range(40):\n    image = cv2.imread(image_path('images',i+1))\n    image_rescaled = rescale(image, 1.0 / 6.0, anti_aliasing=True)\n    label = label_generator(i+1)\n    label_rescaled = rescale(label, 1.0 / 6.0, anti_aliasing=True)\n    x_train[i,:,:,:] = image_rescaled\n    y_train[i,:,:,:] = label_rescaled\n    x_train[40+i,:,:,:] = np.fliplr(image_rescaled)\n    y_train[40+i,:,:,:] = np.fliplr(label_rescaled)\n    x_train[80+1,:,:,:] = np.flipud(image_rescaled)\n    y_train[80+i,:,:,:] = np.flipud(label_rescaled)\n    plt.subplot(8,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(x_train[i,:,:,:])\n    \nfor i in range(20):\n    image = cv2.imread(image_path('images',i+41))\n    image_rescaled = rescale(image, 1.0 / 6.0, anti_aliasing=True)\n    label = label_generator(i+41)\n    label_rescaled = rescale(label, 1.0 / 6.0, anti_aliasing=True)\n    x_test[i,:,:,:] = image_rescaled\n    y_test[i,:,:,:] = label_rescaled\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure()\n#plt.imshow(y_train[i,:,:])\nfor i in range(40):\n    plt.subplot(8,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(y_train[i,:,:,2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# functions for metrics evaluation\n\n# defining a softmax function for our model\ndef depth_softmax(matrix):\n    sigmoid = lambda x: 1 / (1 + K.exp(-x))\n    sigmoided_matrix = sigmoid(matrix)\n    softmax_matrix = sigmoided_matrix / K.sum(sigmoided_matrix, axis=0)\n    return softmax_matrix\n\n# defining a recall function for our model\n# recall means the accuracy of predicting the negative class.\ndef recall(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\n# defining a precision function for our model\n# precision means the accuracy of predicting the positive class\ndef precision(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\n# defining a f1 function for computing the f1 score\n# f1 score  = 2* precision*recall/precision + recall\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n\n'''\nMean Intersection-Over-Union is a common evaluation metric for semantic image segmentation, \nwhich first computes the IOU for each semantic class and then computes the average over classes.\nIOU is defined as follows: IOU = true_positive / (true_positive + false_positive + false_negative).\n'''\ndef mean_iou(y_true, y_pred):\n    prec = []\n    for t in np.arange(0.5, 1.0, 0.05):\n        y_pred_ = tf.to_int32(y_pred > t)\n        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n        K.get_session().run(tf.local_variables_initializer())\n        with tf.control_dependencies([up_opt]):\n            score = tf.identity(score)\n        prec.append(score)\n    return K.mean(K.stack(prec), axis=0)\n\n'''\nBinary cross entropy is just a special case of categorical cross entropy. The equation for binary cross entropy\nloss is the exact equation for categorical cross entropy loss with one output node. For example, binary cross \nentropy with one output node is the equivalent of categorical cross entropy with two output nodes.\n'''\ndef weighted_binary_cross_entropy(y_true, y_pred):\n    w = tf.reduce_sum(y_true)/tf.cast(tf.size(y_true), tf.float32)\n    real_th = 0.5-(1.0/2.0)\n    tf_th = tf.fill(tf.shape(y_pred), real_th) \n    tf_zeros = tf.fill(tf.shape(y_pred), 0.)\n    return (1.0 - w) * y_true * - tf.log(tf.maximum(tf.zeros, tf.sigmoid(y_pred) + tf_th)) + (1- y_true) * w * -tf.log(1 - tf.maximum(tf_zeros, tf.sigmoid(y_pred) + tf_th))\n#return weighted_binary_cross_entropy\n\n# assigning weights to target according to output\ndef class_weighted_pixelwise_crossentropy(target, output):\n    output = tf.clip_by_value(output, 10e-8, 1.-10e-8)\n    weights = [0.8, 0.2]\n    return -tf.reduce_sum(target * weights * tf.log(output))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\n# defining an activation function\ndef BatchActivate(x):\n    # using batch normalization\n    x = BatchNormalization()(x)\n    # using relu activation function\n    x = Activation('relu')(x)\n    return x\n\n# defining a convolutional block\ndef convolution_block(x, filters, size, strides=(1,1), padding='same', activation=True):\n    # using a strides of 1 by 1 with a activation function\n    x = Conv2D(filters, size, strides=strides, padding=padding)(x)\n    if activation == True:\n        x = BatchActivate(x)\n    return x\n\n# defining a residual block\ndef residual_block(blockInput, num_filters=16, batch_activate = False):\n    x = BatchActivate(blockInput)\n    x = convolution_block(x, num_filters, (3,3) )\n    x = convolution_block(x, num_filters, (3,3), activation=False)\n    x = Add()([x, blockInput])\n    if batch_activate:\n        x = BatchActivate(x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# defining our model\n\ndef unet_res(n_classes = 1, start_neurons = 16, DropoutRatio = 0.5, img_height=161,img_width=216):\n    # 101 -> 50\n    # defining the input layer\n    input_layer = Input((img_height, img_width, 3))\n    # padding, initially adding zeros to input layer\n    zero_pad = ZeroPadding2D(padding=((7,8),(4,4)))(input_layer)\n    \n    # starting with convolutional layer\n    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(zero_pad)\n    # adding residual blocks\n    conv1 = residual_block(conv1,start_neurons * 1)\n    conv1 = residual_block(conv1,start_neurons * 1, True)\n    \n    # max pooling applied \n    pool1 = MaxPooling2D((2, 2))(conv1)\n    # dropout to avoid overfitting\n    pool1 = Dropout(DropoutRatio/2)(pool1)\n\n    # 50 -> 25\n    # starting with convolutional layer\n    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(pool1)\n    # adding residual blocks\n    conv2 = residual_block(conv2,start_neurons * 2)\n    conv2 = residual_block(conv2,start_neurons * 2, True)\n    # adding pooling\n    pool2 = MaxPooling2D((2, 2))(conv2)\n    # dropout to avoid overfitting\n    pool2 = Dropout(DropoutRatio)(pool2)\n\n    # 25 -> 12\n    # starting with convolutional layer\n    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(pool2)\n    # adding residual blocks\n    conv3 = residual_block(conv3,start_neurons * 4)\n    conv3 = residual_block(conv3,start_neurons * 4, True)\n    # adding max pooling\n    pool3 = MaxPooling2D((2, 2))(conv3)\n    # adding dropout\n    pool3 = Dropout(DropoutRatio)(pool3)\n\n    # 12 -> 6\n    # starting wit convolutional layer\n    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(pool3)\n    # adding residual blocks\n    conv4 = residual_block(conv4,start_neurons * 8)\n    conv4 = residual_block(conv4,start_neurons * 8, True)\n    # adding max pooling layer\n    pool4 = MaxPooling2D((2, 2))(conv4)\n    # adding dropout\n    pool4 = Dropout(DropoutRatio)(pool4)\n\n    # Middle\n    convm = Conv2D(start_neurons * 16, (3, 3), activation=None, padding=\"same\")(pool4)\n    convm = residual_block(convm,start_neurons * 16)\n    convm = residual_block(convm,start_neurons * 16, True)\n    \n    # 6 -> 12\n    # transpose convolutional layer 4\n    # going backward of convolution operation, it is the core idea of transposed convolution.\n    # used as up sampling method\n    deconv4 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n    # concatinating the convolutional 4 and transposed convolutional layers 4\n    uconv4 = concatenate([deconv4, conv4])\n    # adding dropout\n    uconv4 = Dropout(DropoutRatio)(uconv4)\n    \n    # adding it to the architecture \n    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(uconv4)\n    uconv4 = residual_block(uconv4,start_neurons * 8)\n    uconv4 = residual_block(uconv4,start_neurons * 8, True)\n    \n    # 12 -> 25\n    # transposed convolutional layer 3\n    deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n    #deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"valid\")(uconv4)\n    # adding convolutional layer 3 to transposed convolutional layer 3\n    uconv3 = concatenate([deconv3, conv3])\n    # adding dropout\n    uconv3 = Dropout(DropoutRatio)(uconv3)\n    \n    # adding it to the architecture\n    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(uconv3)\n    uconv3 = residual_block(uconv3,start_neurons * 4)\n    uconv3 = residual_block(uconv3,start_neurons * 4, True)\n\n    # 25 -> 50\n    # transposed convolutional layer 2\n    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n    # adding convolutional layer 2 and transposed convolutional layer 2\n    uconv2 = concatenate([deconv2, conv2])\n    # adding dropout\n    uconv2 = Dropout(DropoutRatio)(uconv2)\n    \n    # adding it to the architecture\n    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(uconv2)\n    uconv2 = residual_block(uconv2,start_neurons * 2)\n    uconv2 = residual_block(uconv2,start_neurons * 2, True)\n    \n    # 50 -> 101\n    # transposed convolutional layer 1\n    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n    #deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"valid\")(uconv2)\n    # adding convolutional layer 1 and transposed convolutional layer 1\n    uconv1 = concatenate([deconv1, conv1])\n    # adding dropout\n    uconv1 = Dropout(DropoutRatio)(uconv1)\n    \n    # adding it to the architecture\n    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(uconv1)\n    uconv1 = residual_block(uconv1,start_neurons * 1)\n    uconv1 = residual_block(uconv1,start_neurons * 1, True)\n    \n    #uconv1 = Dropout(DropoutRatio/2)(uconv1)\n    #output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv1)\n    \n    # output layer\n    output_layer_noActi = Conv2D(n_classes, (1,1), padding=\"same\", activation=None)(uconv1)\n    output_layer = Cropping2D(cropping=((7,8),(4,4)))(output_layer_noActi)\n    \n    # defining the output layer\n    # reshaping the images to defined width and height\n    if n_classes == 1:\n        output_layer = (Reshape((img_height, img_width),\n                          input_shape=(img_height, img_width, 3)))(output_layer)\n    else:\n        output_layer = (Reshape((img_height, img_width, n_classes),\n                          input_shape=(img_height, img_width, 3)))(output_layer)\n    \n    # defining the activation for output layer to be sigmoid\n    output_layer =  Activation('sigmoid')(output_layer)\n    \n    # finalizing our model\n    model = Model(input_layer, output_layer)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# looking at the summary of the training model\n\ntrial_model = unet_res(n_classes = 1, start_neurons=16)\ntrial_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"\n# getting the bg model\nbg_model = unet_res(n_classes=1)\n\n# compiling the model\nbg_model.compile(loss = 'binary_crossentropy',\n             optimizer = 'Adam',\n             metrics = [f1, mean_iou])\n\n# using early stoppinng and reduce learning on plateau to avoid overfitting\ncallbacks_bg = [\n    EarlyStopping(patience=5, verbose=1),\n    ReduceLROnPlateau(patience=3, verbose=1),\n    ModelCheckpoint('model-bg-cwfid.h5', verbose=1, save_best_only=True)\n]\n\n# feeding training data to bg model\nmodel_bg_history = bg_model.fit(x = x_train,\n                         y = y_train[:,:,:,2],\n                         batch_size = 5,\n                         epochs = 20,\n                         verbose = 1,\n                         validation_split = 0.3,\n                          callbacks = callbacks_bg\n                         )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\n# getting the weed model\nweed_model = unet_res(n_classes=1)\n\n# compiling the compile\n# optimizer used is adam optimizer\n# metrics used are f1 and mean_iou as defined above\nweed_model.compile(loss = 'binary_crossentropy',\n             optimizer = 'Adam',\n             metrics = [f1, mean_iou])\n\n# using early stopping to stop when the model stops learning\ncallbacks_weed = [\n    EarlyStopping(patience=5, verbose=1),\n    ReduceLROnPlateau(patience=3, verbose=1),\n    ModelCheckpoint('model-weed-cwfid.h5', verbose=1, save_best_only=True)\n]\n\n# feeding the model with training data\nmodel_weed_history = weed_model.fit(x = x_train,\n                         y = y_train[:,:,:,1],\n                         batch_size = 5,\n                         epochs = 20,\n                         verbose = 1,\n                         validation_split = 0.2,\n                          callbacks = callbacks_weed\n                         )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\n# getting the crop model\ncrop_model = unet_res(n_classes=1)\n\n# compiling the crop_model with adam optimizer\ncrop_model.compile(loss = 'binary_crossentropy',\n             optimizer = 'Adam',\n             metrics = [f1, mean_iou])\n\n# using early stopping to stop when the model stops learning\ncallbacks_crop = [\n    EarlyStopping(patience=5, verbose=1),\n    ReduceLROnPlateau(patience=3, verbose=1),\n    ModelCheckpoint('model-crop-cwfid.h5', verbose=1, save_best_only=True)\n]\n\n# feeding training data to crop_model\nmodel_crop_history = crop_model.fit(x = x_train,\n                         y = y_train[:,:,:,0],\n                         batch_size = 5,\n                         epochs = 20,\n                         verbose = 1,\n                         validation_split = 0.3,\n                          callbacks = callbacks_crop\n                         )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# predictions for all the three models\n\npred_bg = bg_model.predict(x_test)\npred_weed = weed_model.predict(x_test)\npred_crop = crop_model.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# looking at the images predicted by all model for weeds and crops.\n\ni = 15\nplt.figure()\nplt.subplot(2,3,1)\nplt.imshow(pred_bg[i,:,:])\nplt.subplot(2,3,4)\nplt.imshow(y_test[i,:,:,2])\nplt.subplot(2,3,2)\nplt.imshow(pred_weed[i,:,:])\nplt.subplot(2,3,5)\nplt.imshow(y_test[i,:,:,1])\nplt.subplot(2,3,3)\nplt.imshow(pred_crop[i,:,:])\nplt.subplot(2,3,6)\nplt.imshow(y_test[i,:,:,0])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\n# combining all the model\nfull_model = unet_res(n_classes=3)\n\n# compiling the model\nfull_model.compile(loss = 'binary_crossentropy',\n             optimizer = 'Adam',\n             metrics = [f1, mean_iou])\n\n# using early stopping, reduce-learning rate on plateau to avoid over fititng\ncallbacks_full = [\n    EarlyStopping(patience=8, verbose=1),\n    ReduceLROnPlateau(patience=5, verbose=1),\n    ModelCheckpoint('model-full-cwfid.h5', verbose=1, save_best_only=True)\n]\n\n\n# feeding training data to the model \nmodel_full_history = full_model.fit(x = x_train,\n                         y = y_train,\n                         batch_size = 5,\n                         epochs = 30,\n                         verbose = 1,\n                         validation_split = 0.2,\n                          callbacks = callbacks_full\n                         )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# predicting for the full combined model\npred_full = full_model.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# looking at the results predicted by our model\n\ni = 15\nplt.figure()\nplt.subplot(2,3,1)\nplt.imshow(pred_full[i,:,:,2])\nplt.subplot(2,3,4)\nplt.imshow(y_test[i,:,:,2])\nplt.subplot(2,3,2)\nplt.imshow(pred_full[i,:,:,1])\nplt.subplot(2,3,5)\nplt.imshow(y_test[i,:,:,1])\nplt.subplot(2,3,3)\nplt.imshow(pred_full[i,:,:,0])\nplt.subplot(2,3,6)\nplt.imshow(y_test[i,:,:,0])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}